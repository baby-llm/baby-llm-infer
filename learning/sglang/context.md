# 代码结构

```Engine.py

```

```Scheduler.py

```

# 背景知识

## DP和TP的区别是什么

数据并行 (Data Parallelism, DP) vs 张量并行 (Tensor Parallelism, TP)
这两种是常见的模型并行技术，用于在多个计算设备（通常是 GPU）上训练或运行大型深度学习模型。

张量并行 (Tensor Parallelism, TP):

核心思想: 将模型中的单个大层（例如 Transformer 中的 FFN 层或 Attention 层）的计算和参数切分到多个设备上。每个设备只持有和计算模型层的一部分（一个“分片”）。
工作方式:
权重矩阵被切分（例如按列或按行）。
输入激活（张量）也被相应切分或复制。
每个设备在其分片上执行计算。
通常需要在设备间进行通信（如 all-gather 或 reduce-scatter）来组合中间结果或最终输出。
优点: 可以运行单个层或整个模型都无法放入单个设备内存的超大模型。
缺点: 需要设备间有高带宽、低延迟的连接（如 NVLink），通信开销可能较大。实现相对复杂。
在此代码中: 当 dp_size == 1 时，run_scheduler_process 负责管理一个 TP rank（模型的一个分片）。多个这样的进程协同工作，共同完成整个模型的推理。
数据并行 (Data Parallelism, DP):

核心思想: 将整个模型完整地复制到多个设备上。每个设备处理输入数据的不同部分（不同的 mini-batch）。
工作方式:
输入数据被分成多个子批次。
每个设备使用其上的模型副本独立处理一个子批次。
（训练时）计算梯度，然后通过通信（如 all-reduce）聚合所有设备上的梯度，最后用聚合后的梯度更新每个设备上的模型副本。
（推理时）每个副本独立处理请求，可以显著提高总吞吐量。
优点: 实现相对简单，可以有效提高训练或推理的吞吐量。对设备间带宽要求通常低于 TP。
缺点: 每个设备都需要存储整个模型的副本，因此无法运行单设备内存放不下的模型。
在此代码中: 当 dp_size > 1 时，run_data_parallel_controller_process 启动。这个控制器负责协调多个模型副本（每个副本可能内部还使用了 TP）。它会将收到的请求分发给不同的模型副本进行处理，从而实现数据并行。
总结:

TP 切分模型层，DP 复制整个模型。
TP 解决模型过大放不下的问题，DP 解决处理速度不够快的问题。
它们可以结合使用：例如，使用 TP 将一个巨大的模型切分到 4 个 GPU 上，然后将这个 4-GPU 的 TP 单元作为一个整体，再进行 DP 复制到另外几组 4-GPU 上，以同时提高处理能力和吞吐量。此代码通过 dp_size 和 tp_size 参数以及相应的启动逻辑来支持这两种模式及其组合。

# SGlang如何管理kvcache

GitHub Copilot
好的，我们来分解一下 LLM 推理流程中 CPU、GPU、内存（RAM）和磁盘之间的数据流，并结合您提供的 KV 缓存管理代码。
